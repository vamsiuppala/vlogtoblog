used this particular model um or maybe it's not I actually have no idea because the problem is these metrics are not particularly well aligned with real life usage um for all kinds of reasons and also sometimes you get something called leakage which means that sometimes some of the questions from these things actually leaks through to some of the training sets so you can get as a rule of thumb what to use from here but you should always try things um and you can also say you know these ones are all the 70b here that tells you how big it is so this is a 70 billion parameter model um so generally speaking for the kinds of gpus you we're talking about you'll be wanting no bigger than 13B and quite often 7B um so let's see if we've confined here the 13B model for example um all right so you can find models to try out from things like this leaderboard um and there's also a really great leaderboard called fast eval which I like a lot because it focuses on some more sophisticated evaluation methods such as this Chain of Thought evaluation method so I kind of trust these a little bit more and these are also GSM 8K is a difficult math benchmark uh big bench hard um so forth so yeah so you know stable Beluga 2 Wizard math 13B dolphin Lima 13B et cetera these would all be good options um yeah so you need to pick a model and at the moment nearly all the good models are based on metas llama too so when I say based on what does that mean well what that means is this model here llama 2 7B so it's a llama model that's that's just the name meta called it this is their version two of llama this is their seven billion size one it's the smallest one that they make and specifically these weights have been created for hugging face so you can load it with the hugging face Transformers and this model has only got As far as here it's done the language model of pre-trading it's done none of the instruction tuning and none of the rlhf um so we would need to fine tune it to really get it to do much useful so we can just say Okay create a automatically create the appropriate model for language models so cause or LM is basically refers to that ULM fit stage one process or stage two in fact so we've got the pre-trained model from this name metal alarm element two blah blah okay now um generally speaking we use 16-bit floating Point numbers nowadays but if you think about it 16 bit is two bytes so 7B times two it's going to be 14 gigabytes just to load in the weights so you've got to have a decent model to be able to do that perhaps surprisingly you can actually just cast it to 8-bit and it still works pretty well thanks to something called discretization so let's try that so remember this is just a language model looking only complete sentences we can't ask it a question and expect a great answer so let's just give it the start of a sentence Jeremy how it is a and so we need the right tokenizer so this will automatically create the right kind of tokenizer for this model we can grab the tokens as Pi torch here they are and just to confirm if we decode them back again we get back the original plus a special token to say this is the start of a document and so we can now call generate so generate will um Auto regressively so call the model again and again passing its previous result back as the next as the next input and I'm just going to do that 15 times so this is you can you can write this for Loop yourself this isn't doing anything fancy in fact I would recommend writing this yourself to make sure that you know how that it all works okay um we have to put those tokens on the GPU and at the end I recommend putting them back onto the CPU the result and here are the tokens not very interesting so we have to decode them using the tokenizer and so the first 25 sorry first 15 tokens are Jeremy Howard is a 28 year old Australian AI researcher and entrepreneur okay well 28 years old is not exactly correct but we'll call it close enough I like that thank you very much llama 7B So Okay so we've got a language model completing sentences it took one in the third seconds and that's a bit slower than it could be because we used 8-bit if we use 16 bit there's a special thing called B float 16 which is a really great 16-bit floating Point format that's used usable on any somewhat recent GPS Nvidia GPU now if we use it it's going to take twice as much RAM as we discussed but look at the time it's come down to 390 milliseconds um now there is a better option still than even that there's a different kind of discretization called gptq 