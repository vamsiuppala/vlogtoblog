hi I am Jeremy Howard from fast.ai and this is a hacker's guide to language models when I say a hacker's guide what we're going to be looking at is a code first approach to understanding how to use language models in practice so before we get started we should probably talk about what is a language model I would say that this is going to make more sense if you know the kind of basics of deep learning if you don't I think you'll still get plenty out of it and there'll be plenty of things you can do but if you do have a chance I would recommend checking out course.fast.ai which is a free course and specifically um if you could at least kind of watch if not work through the first five lessons that would get you to a point where you understand all the basic fundamentals of deep learning that will make this this lesson tutorial make even more sense maybe I shouldn't call this a tutorial it's more of a quick run through so I've got to try to run through all the basic ideas of language models how to use them both open source ones and open AI based ones and it's all going to be based using Code as much as possible um so let's start by talking about what a language model is and so as you might have heard before a language model is something that knows how to predict the next word of a sentence or knows how to fill in the missing words of a sentence and we can look at an example of one open AI has a language model text DaVinci 003 and we can play with it by passing in some words and ask it to predict what the next words might be so if we pass in when I arrived back at the panda breeding facility after the extraordinary reign of live frogs I couldn't believe what I saw I just came up with that yesterday and I thought what might happen next so kind of fun for Creative brainstorming uh there's a nice site called nat.dev Nat dot let Dev lets us play with a variety of language models and here I've selected text DaVinci 003 and I'll hit submit and it starts printing stuff out the pandas were happily playing and eating the frogs that had fallen from the sky there's an amazing sight to see these animals taking advantage of such a unique opportunity first after quick measures to ensure the safety of the pandas and the frogs so there you go that's what happened after the extraordinary reign of live frogs at the panda breeding facility uh you'll see here that I've enabled show probabilities which is a thing in that.dev where it shows um well let's take a look it's pretty likely the next word here is going to be the and after this since we're talking about a panda breeding facility it's going to be Panda's were and what were they doing well they could have been doing a few things they could have been doing something happily or the pandas were having the pandas were out the pandas were playing so it picked the most likely uh it thought it was 20 likely it's going to be happily and what were they happily doing could have been playing hopping eating and so forth so they're eating the frogs that and then had almost certainly so you can see what it's doing at each point is it's predicting the probability of a variety of possible next words and depending on how you set it up it will either pick the most likely one every time or you can change muck around with things like P values and temperatures to change what comes up so at each time then it'll give us a different result and this is kind of fun frogs perched on the heads of some of the pandas it was an amazing sight etc etc okay so that's what a language model does um now you might notice here it hasn't predicted pandas it's predicted panned and then separately us okay after Panda it's going to be us so it's not always a whole word here it's an and then harmed oh actually it's unha mood so you can see that it's not always predicting words specifically what it's doing is predicting tokens uh tokens are either whole words or sub word units pieces of a word or it could even be punctuation or numbers or so forth um so let's have a look at how that works so for example we can use the actual um it's called tokenization to create tokens from us from a uh from a string we can use the same tokenizer that GPT uses by using tick token and we can specifically say we want to use the same tokenizer that that model text eventually double O three uses and so for example when I earlier tried this it talked about the Frog splashing and so I thought I'll include data we'll encode they are splashing and the result is a bunch of numbers and what those numbers are they'd basically just lookups into a vocabulary that openai in this case created and if you train your own models you'll be automatically creating or your code will create and if I then decode those it says oh these numbers are they space r space spool hashing and so put that all together they are splashing so you can see that the start of a word is give me the space before it is also being encoded here so these um language models are quite neat that they can work at all but they're not of themselves really designed to do anything um uh let me explain um the basic idea of what chat GPT gpt4 Bard Etc are doing comes from a paper which describes an algorithm that I created back in 2017 called ULM fit and Sebastian Rooter and I wrote a paper up describing the ULM fit approach which was the one that basically laid out what everybody's doing how this system works and the system has three steps step one is language model training but you'll see this is actually from the paper we actually described it as pre-training now what language model pre-training does is this is the thing which predicts the next word of a sentence and so in the original ULM fit paper so the algorithm I developed in 2017 then Sebastian Rooter and I wrote it up in 2018 early 2018 what I originally did was I trained this language model on Wikipedia now what that meant is I took a neural network um and a neural network is just a function if you don't know what it is it's just a mathematical function that's extremely flexible and it's got lots and lots of parameters and initially it can't do anything but using stochastic gradient descent or SGD you can teach it to do almost anything if you give it examples and so I gave it lots of examples of sentences from Wikipedia so for example from the Wikipedia article for the birds the birds is a 1963 American Natural horror natural horror Thriller film produced and directed by Alfred and then it would stop and so then the model would have to guess what the next word is and if it guest Hitchcock it would be rewarded and if it gets guessed something else it would be penalized and effectively basically it's trying to maximize those rewards it's trying to find a set of weights for this function that makes it more likely that it would predict Hitchcock and then later on in this article it reads from Wikipedia at a previously dated Mitch but ended it due to Mitch's cold overbearing mother Lydia who dislikes any woman in mitches now you can see that filling this in actually requires being pretty thoughtful because there's a bunch of things that like kind of logically could go there like a woman could be in Mitch's closet could be in which is house and so you know you could probably guess in the Wikipedia article describing the plot of the birds it's actually any woman in Mitch's life now to do a good job of solving this problem as well as possible of guessing the next word of sentences the neural network is gonna have to learn a lot of stuff about the world it's going to learn that there are things called objects that there's a thing called time that objects react to each other over time that there are things called movies that movies have directors that there are people that people have names and so forth and that a movie director is Alfred Hitchcock and he directed horror films and um so on and so forth it's going to have to learn extraordinary amount if it's going to do a really good job of predicting the next word of sentences now these neural networks specifically are deep neural networks so this is deep learning and in these deep neural networks which have um when when I created this I think it had like 100 million parameters nowadays they have billions of parameters um it's got the ability to create a rich hierarchy of abstractions and representations which it can build on and so this is really the the key idea behind neural networks and language models is that if it's going to do a good job of being able to predict the next word of any sentence in any situation it's going to have to know an awful lot about the world it's going to have to know about how to solve math questions or figure out the next move in a chess game or recognize poetry and so on and so forth now nobody said it's going to do a good job of that so it's a lot of work to find to create and train a model that is good at that but if you can create one that's good at that it's going to have a lot of capabilities internally that it would have to be a drawing on to be able to do this effectively so the key idea here for me is that this is a form of compression and this idea of the relationship between compression and intelligence goes back many many decades and the basic idea is that yeah if you can guess what words are coming up next then effectively you're compressing all that information down into a neural network um now I said this is not useful of itself well why do we do it well we do it because we want to pull out those capabilities and the way we pull out those capabilities is we take two more steps the second step is we do something called language model fine tuning a language model fine tuning we are no longer just giving it all of Wikipedia or nowadays we don't just give it all of Wikipedia but in fact a large chunk of the internet is fed to pre-training these models in the fine tuning stage we feed it a set of documents a lot closer to the final task that we want the model to do but it's still the same basic idea it's still trying to predict the next word of a sentence after that we then do a final classifier fine tuning and then the classifier fine-tuning this is this is the kind of end task we're trying to get it to do now nowadays these two steps are very specific approaches are taken for the step two the step B the language model fine tuning people nowadays do a particular kind called instruction tuning the idea is that the task we want most of the time to achieve is solve problems answer questions and so in the instruction tuning phase we use data sets like this one this is a great data set called openalker created by a fantastic open source group and and it's built on top of something called the flan collection and you can see that basically there's all kinds of different questions in here so this four gigabytes of of questions and context and so forth and each one generally has a question or an instruction or a request and then a response here are some examples of instructions I think this is from the flan data set if I remember correctly so for instance it could be does the sentence in the Iron Age answer the question the period of time from 1200 to 1000 BCE is known as what choice is one yes or no and then the language model is meant to write one or two as appropriate for yes or no or it could be uh things about I think this is from a music video who is the girl in more than you know answer and then it would have to write the correct name of the remember model or dancer or whatever from um from that music video and so forth so it's still doing language modeling so fine-tuning and pre-training are kind of the same thing but this is more targeted now not just to be able to fill in the missing parts of any document from the internet um but to fill in the words necessary to to answer questions to do useful things okay so that's instruction tuning and then step three which is the classifier fine tuning nowadays there's generally various approaches such as reinforcement learning from Human feedback and others which are basically giving humans or sometimes more advanced models multiple answers to a question such as here are some from a reinforcement lighting from Human feedback paper I can't remember which one I got it from list five ideas for how to regain enthusiasm for my career and so the model will spit out two possible answers or it'll have a less good model and a more good model and then a human or a better model will pick which is best and so that's used for the the final fine tuning Stitch so all of that is to say um although you can download pure language models from the internet um they're not generally that useful of their on their own until you've fine-tuned them now you don't necessarily need step C nowadays actually people are discovering that maybe just step B might be enough it's still a bit controversial Okay so when we talk about a language bottle um where we could be talking about something that's just been pre-trained something that's been fine-tuned or something that's gone through something like rlhf all of those things are generally described nowadays as language models so my view my view is that if you are going to be good at language modeling in any way then you need to start by being a really effective user of language models and to be a really effective user of language models you've got to use the best one that there is and currently so what are we up to September 2023 the best one is by far gpt4 this might change sometime in the not too distant future but this is right now gpt4 is the recommendation strong strong recommendation now you can use GPT for by paying 20 bucks a month to open Ai and then you can use it a whole lot it's very hard to to run out of credits I find now what can GPT do it's interesting and instructive in my opinion to start with the very common views you see on the internet or even in Academia about what it can't do so for example there was this paper you might have seen GPT for can't reason which describes a number of uh empirical analysis done of 25 diverse reasoning problems and found it that it was not able to solve them and it's utterly incapable of reasoning so I always find you've got to be a bit careful about reading stuff like this because I just talked the first three that I came across in that paper and I gave them to gpt4 um and by the way something very useful in gpt4 is you can click on the the 