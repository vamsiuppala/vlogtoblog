open Orca prompt that has that prompt format and so now we can say okay who is Jeremy Howard and now I've become British which is kind of true I was born in England but I moved to Australia uh professional poker player no definitely not that uh co-founding several companies including first.ai also kaggle okay so not bad yeah it was acquired by Google was it 2017 probably something around there okay so you can see we've got our own models giving us some pretty good information how do we make it even better you know because it's it's it's still hallucinating you know um and you know llama two I think has been trained with more up-to-date information than gpt4 it doesn't have the September 2021 cut off um but it you know it's still got a knowledge cut off you know we would like to use the most up-to-date information we want to use the right information to answer these questions as well as possible so to do this we can use something called retrieval augmented generation so what happens with retrieval augmented generation is when we take the question we've been asked like who is Jeremy held and then we say okay let's try and search for documents that may help us answer that question so obviously we would expect for example Wikipedia to be useful and then what we do is we say okay with that information let's now see if we can tell the language model about what we found and then have it answer the question so let me show you so let's actually grab a Wikipedia python package we will scrape Wikipedia grabbing the Jeremy Howard web page and so here's the start of the Jeremy Howard Wikipedia page it has 613 words now generally speaking these open source models will have a context length of about two thousand or four thousand so the context length is how many tokens Can it handle so that's fine it'll be able to handle this web page and what we're going to do is we're going to ask it the question so we're going to have here question and with a question but before it we're going to say answer the question with the help of the context we're going to provide this to the language model and we're going to say context and they're going to have the whole web page so suddenly now our question is going to be a lot bigger our prompt right so our prompt now contains the entire web page the whole Wikipedia page followed by a question and so now it says Jeremy how does an Australian data scientist Edge entrepreneur an educator known for his work in deep learning co-founder of fast AI teaches courses develops software conducts research used to be yeah okay it's perfect right so it's actually done a really good job like if somebody asked me to send them a you know 100 word bio uh that would actually probably be better than I would have written myself and you'll see even though I asked for 300 tokens it actually got sent back the end of stream token and so it knows to stop at this point um well that's all very well but how do we know to pass in the Jeremy Howard Wikipedia page well the way we know which Wikipedia page to pass in is that we can use another model to tell us which web page or which document is the most useful for answering a question and the way we do that is we we can use something called sentence Transformer and we can use a special kind of model that specifically designed to take a document and turn it into a bunch of activations where two documents that are similar will have similar activations so let me just let me show you what I mean what I'm going to do is I'm going to grab just the first paragraph of my Wikipedia page and I'm going to grab the first paragraph of Tony Blair's Wikipedia page okay so we're pretty different people right this is just like a really simple small example and I'm going to then call this model so I'm going to say encode and I'm going to encode my Wikipedia first paragraph Tony Blair's first paragraph and the question which was who is Jeremy Howard and it's going to pass back a 384 long vector of embeddings for the question for me and for Tony Blair and what I can now do is I can calculate the similarity between the question and the Jeremy Howard Wikipedia page and I can also do it for the question versus the Tony Blair Wikipedia page and as you can see it's higher for me and so that tells you that if you're trying to figure out what document to use to help you answer this question better off using the Jeremy Howard Wikipedia page than the Tony Blair Wikipedia pitch foreign so if you had a few hundred documents you were thinking of using to give back to the model as context to help it answer a question you could literally just pass them all through to encode go through each one one at a time and see which is closest when you've got thousands or millions of documents you can use something called a vector database where basically as a one-off thing you go through and you encode all of your documents and so in fact um there's there's lots of pre-built systems for this um here's an example of one called H2O GPT and this is just something that I've got um that I've got running here on my computer it's just an open source thing written in Python and sitting here running on Port 7860 and so I just gone to localhost 7860 and what I did was I just uploaded I just clicked upload and I've wrapped last uploaded a bunch of papers in fact I might be able to see it better yeah here we go a bunch of papers and so you know we could look at uh let me search yeah I can so for example we can look at the ULM fit paper that uh so bruter and I did and you can see it's taken the PDF and turned it into slightly crappily a text format and then it's created an embedding for each you know each section so I could then um ask it you know what is ULM fit and I'll hit enter and you can see here it's now actually saying based on the information provided in the context so it's showing us it's been given some context what context did it get so here are the things that it found right so it's being sent this context so this is kind of citations performance by leveraging the knowledge and adapting it to the specific task at hand um how what techniques be more specific does ULM fit uh let's see how it goes okay there we go so here's the three steps pre-trained fine-tune fine tune cool um so you can see it's not bad right um it's not amazing like you know the context in this particular case is pretty small um and it's and in particular if you think about how that embedding thing worked you can't really use like the normal kind of follow-up so for example um if I so it says fine tuning a classifier so I could say what classifier is used now the problem is that there's no context here being sent to the embedding model so it's actually going to have no idea I'm talking about new lmfit so generally speaking it's going to do a terrible job yeah I see it says it's used as a Roberta model but it's not but if I look at the sources it's no longer actually referring to Howard and Rooter so anyway you can see the basic idea this is called retrieval augmented generation Reg um and it's a it's a Nifty approach but you have to do it with with some care 