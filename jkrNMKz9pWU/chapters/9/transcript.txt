um and so there are lots of these uh private GPT things out there um actually the H2O GPT web page does a fantastic job of listing lots of them and comparing so as you can see if you want to run a private GPT there's no shortage of options and you can have your retrieval augmented generation I haven't tried I've only tried this one H2O GPT I don't love it it's all right um good so finally I want to talk about what's perhaps the most interesting option we have which is to do our own fine tuning and fine tuning is cool because rather than just retrieving documents which might have useful context we can actually change our model to behave based on the documents that we have available and I'm going to show you a really interesting example of fine tuning here what we're going to do is we're going to fine tune using this um no SQL data set and it's got examples of like a a schema for a table in a database a question and then the answer is the correct SQL to solve that question using that database schema and so I'm hoping we could use this to create a um you know I kind of it could be a hand to use a handy tool for for business users where they type some English question and SQL generated for them automatically don't know if it actually work in practice or not but this is just a little fun idea I thought we'd try out um I know there's lots of uh startups and stuff out there trying to do this more seriously but this is this is quite cool because it actually got it working today in just a couple of hours so what we do is we use the hugging face data sets library and what that does just like the hugging face Hub has lots of models stored on it hacking face data sets has lots of data sets stored on it and so instead of using Transformers which is what we use to grab models we use data sets and we just pass in the name of the person and the name of their repo and it grabs the data set and so we can take a look at it and it just has a training set with features and so then I can have a look at the training set so here's an example which looks a bit like what we've just seen so what we do now is we want to fine-tune a model now we can do that in in a notebook from scratch takes I don't know 100 or so lines of code it's not too much but given the time constraints here and also like I thought why not why don't we just use something that's ready to go so for example there's something called Axolotl which is quite nice in my opinion here it is here lovely another very nice open source piece of software and uh again you can just pip install it and it's got things like gptq and 16 bit and so forth ready to go and so what I did was a um it basically has a whole bunch of examples of things that it already knows how to do it's got llama 2 examples so I copied the Llama 2 example and I created a SQL example so basically just told it this is the path to the data set that I want this is the type um and everything else pretty much I left the same and then I just ran this command which is from there read me accelerate launch Axolotl passed in my yaml and that took about an hour on my GPU and at the end of the hour it had created a q Laura out directory Q stands for quantize that's because I was creating a smaller quantized model Laura I'm not going to talk about today but Laura is a very cool thing that basically another thing that makes your models smaller and also handles I can use bigger models on smaller gpus for training um so uh I trained it and then I thought okay let's uh create our own one so we're going to have this context and um this question get the count of competition hosts by theme and I'm not going to pass it an answer so I'll just ignore that so again I've found out what prompt they were using um and created a SQL prompt function and so here's what I've got to do use the following contextual information to answer the question context create tables there's the context question list or competition host sorted in ascending order and then I tokenized that chord generate and the answer was select count hosts kind of theme from Farm competition Group by theme that is correct 