where a model is carefully optimized to work with uh four or eight or other you know lower Precision data automatically and um this particular person known as the bloke is fantastic at taking popular models running that optimization process and then uploading the results back to hacking face so we can use this gptq version and internally this is actually going to use I'm not sure exactly how many bits this particular one is I think it's probably going to be four bits but it's going to be much more optimized um and so look at this 270 milliseconds it's actually faster than 16 bit even though internally it's actually casting it up to 16 bit each layer to do it and that's because there's a lot less memory moving around and to confirm in fact what we could even do now is we go up to 13B easy and in fact it's still faster than the 7B now that we're using the gptq version so this is a really helpful tip so let's put all those things together the tokenizer the generate the batch decode we'll call this gen for Generate and so we can now use the 13B GPT key model and let's try this Jeremy Howard is a so it's got to 50 tokens so fast 16-year veteran of Silicon Valley co-founder of cargo a Marketplace or predictive model here's company kaggle.com has become the data science competitions what I don't know I was going to say but anyway it's on the right track I was actually there for 10 years not 16 but that's all right um okay so this is looking good um but probably a lot of the time we're going to be interested in you know asking questions or using instructions so stability AI has this nice series called stable Beluga including a small 7B one and other bigger ones and these are all based on llama2 but these have been instruction tuned they might even have been RL hdf but I can't remember now um so we can create a stable Beluga model and now something really important that I keep forgetting everybody keeps forgetting is during the instruction tuning process during the instruction tuning process the instructions that are passed in actually uh um they don't just appear like this they actually always are in a particular format and the format Believe It or Not changes quite a bit from from fine tune to fine tune and so you have to go to the web page for the model and scroll down to found out what the prompt format is so here's the prompt format so I generally just copy it and then I paste it into python which I did here and created a function called make prompt that used the exact same format that it said to use and so now if I want to say who is Jeremy Howard I can call Jen again that was that function I created up here and make the correct prompt from that question and then it returns back okay so you can see here or this prefix this is a system instruction this is my question and then the assistant says Jeremy Howard's an Australian entrepreneur computer scientist co-founder of machine learning and deep Learning Company faster AI okay so this one's actually all correct so it's getting better by using an actual instruction tune model um and so we could then start to scale up so we could use the 13B and in fact uh we looked briefly at this open Orca data set earlier so llama2 has been fine-tuned on Oakman Orca and then also fine-tuned on another really great data set called platypus and so the whole thing together is the open Orca platypus and then this is going to be the bigger 13B gptq means it's going to be quantized so that's got a different format okay a different prompt format so again we can scroll down and see what the prompt format is there it is okay and so we can create a function called make 