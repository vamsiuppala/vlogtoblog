share button and you'll get something that looks like this and this is really handy so here's an example of something from the paper that said gpt4 can't do this Mabel's heart rate at 9 00 am was 75 beats per minute her blood pressure at 7 pm was 120 over 80. she died 11 p.m while she arrive at noon so of course you're human we know obviously she must be and GPT forces Hmm this appears to be a riddle not a real inquiry into medical conditions uh here's a summary of the information and yeah it sounds like Mabel was alive at noon so that's correct uh this was the second one I tried from the paper that says gpt4 can't do this and I found actually gpt4 can do this um and it said that gpt4 can't do this and I found gpt4 can do this now um I mentioned this to say gpt4 is probably a lot better than you would expect if you've read all this um stuff on the internet about all the dumb things that it does um almost every time I see on the internet saying something something that GPT 4 can't do I check it and it turns out it does this one was just last week Sally a girl has three brothers each brother has two sisters how many sisters does Sally have so have a think about it and so gpt4 says okay Sally's counted as one system each of her brothers if each brother has two sisters that means there's another sister in the picture apart from salary so Sally has one sister okay correct um and then this one I got sort of like three or four days ago this is a common view that language models can't track things like this see is the riddle I'm in my house on top of my chair in the living room is a coffee cup inside the coffee cup is a thimble inside the thimble is a diamond I moved the chair to the bedroom I put the coffee cup on the bed I turned the cup upside down then I return it upside up Place The Coffee Cup on the counter in the kitchen where's my diamond and so gpt4 says yeah okay you turned it upside down so probably the diamond fell out so therefore the diamond is in the bedroom where it fell out okay correct um why is it that people are claiming that gpt4 can't do these things we can well the reason is because I think on the whole they are not aware of how gpt4 was trained gpt4 was not trained at any point to give correct answers gpt4 was trained initially to give most likely next words and there's an awful lot of stuff on the internet where the most rare documents are not describing things that are true there could be fiction there could be jokes there could be just stupid people don't saying dumb stuff so this first stage does not necessarily give you correct answers the second stage with the instruction tuning uh also like it's it's it's trying to give correct answers but part of the problem is that then in the stage where you start asking people which answer do they like better people tended to say in these uh in these things that they prefer more confident answers and they often were not people who were trained well enough to recognize wrong answers so there's lots of reasons that the that the you know SGD weight updates from this process for stuff like gpt4 don't particularly or don't entirely reward correct answers but you can help it want to give you correct answers if you think about the LM pre-training what are the kinds of things in a document that would suggest oh this is going to be high quality information and so you can actually Prime gpt4 to give you high quality information by giving it custom instructions and what this does is this is basically text that is prepended to all of your queries and so you say like oh you're brilliant at reasoning so like okay that's obviously or to prime it to give good answers um and then try to work against the fact that um the the rlhf uh folks uh preferred confidence just tell it no tell me if there might not be a correct answer also the way that the text is generated is it literally generates the next word and then it puts all that whole lot back into the bottle and generates the next next word puts that all back in the model generates the next next word and so forth that means the more words it generates the more computation it can do and so I literally I tell it that right and so I say first spend a few sentences explaining background context Etc so this uh custom instruction um allows it to solve more challenging problems and you can see the difference here's what it looks like for example if I say how do I get a count of rows grouped by value in pandas and it just gives me a whole lot of information which is actually it thinking so I just skip over it and then it gives me the answer and actually in my uh um custom instructions I actually say if the request begins with VV actually make it as concise as possible and so it kind of goes into brief mode and here's brief mode how do I get the group this is the same thing but with VV at the start and it just spits it out now in this case it's a really simple question so I didn't need time to think so hopefully that gives you a sense of how to get language models to give good answers you have to help them and if you if it's not working it might be user error basically but having said that there's plenty of stuff that language models like gpt4 can't do one thing to think carefully about is does it know about itself can you ask it what is your context length how were you trained what Transformer architecture are you based on any one of these stages did it have the opportunity to learn any of those things well obviously not at the pre-training stage nothing on the internet existed during GPT 4's training saying how gpt4 was trained right uh probably Ditto in the instruction tuning probably Ditto in the rlhf so in general you can't ask for example a language model about itself now again because of the rlhf it'll want to make you happy by giving your opinionated answers so it'll just spit out the most likely thing it thinks with great confidence this is just a general kind of hallucination right so hallucinations is just this idea that the language model wants to complete the sentence and it wants to do it in an opinionated way that's likely to make people happy um it doesn't know anything about URLs it really hasn't seen many at all I think a lot of them if not all of them pretty much were stripped out so if you ask it anything about like what's at this webpage again it'll generally just make it up um and it doesn't know at least gpt4 doesn't know anything after September 2021 um because the um information it was pre-trained on was from that time period September 2021 and before called the knowledge cut off so here's some things it can't do um Steve Newman sent me this good example of something that it can't do um here is a logic puzzle I need to carry a cabbage a goat and a wolf across a river I can only carry one item at a time I can't leave the goat with a cabbage I can't leave the cabbage with the wolf how do I get everything across to the other side now the problem is this looks a lot like something called the classic River Crossing puzzle so classic in fact that it has a whole Wikipedia page about it and in the classic puzzle the wolf would eat the goat or the goat would eat the cabbage now in in Steve's version he changed it the goat would eat the cabbage and the Wolf would eat the cabbage but the wolf won't eat the goat so what happens well very interestingly gpt4 here is entirely overwhelmed by the language model training it's seen this puzzle so many times it knows what word comes next so it says oh yeah I take the goat across the road across the river and leave it on the other side leaving the wolf with a cabbage but we're just told you can't leave the wolf with a cabbage so it gets it wrong now the thing is though you can encourage gpt4 or any of these language models to try again so during the instruction tuning an R lhf they're actually fine-tuned with multi-stage conversations so you can give it a multi-stage conversation repeat back to me the constraints I listed what happened after Step One is a constraint violated oh yeah yeah yeah I made a mistake okay my new attempt instead of taking the goat across the river and leaving it on the other side is I'll take the code across the river and leave from the other side it's done the same thing um oh yeah I did do the same thing okay I'll take the wolf across well now the goats with the Cabbage that still doesn't work oh yeah that didn't work out uh sorry about that instead of taking the goat across the other side I'll take the goat across the other side okay what's going on here right this is terrible well one of the problems here is that not only is on the Internet it's so common to see this particular goat puzzle that it's so confident it knows what the next word is also on the internet when you see stuff which is stupid on a web page it's really likely to be followed up with more stuff that is stupid once gpt4 starts being wrong it tends to be more and more wrong it's very hard to turn it around to start it making it be right so you actually have to go back and there's actually a an edit button on these chats um and so what you generally want to do is if it's made a mistake is don't say oh here's more information to help you fix it but instead go back and click the edit and change it here and so this time it's not going to get confused so in this case actually fixing Steve's example takes quite a lot of effort but I think I've managed to get it to work eventually and I actually said oh sometimes people read things too quickly they don't notice things it can trick them up then they apply some pattern get the wrong answer you do the same thing by the way so I'm going to trick you so before you about to get tricked make sure you don't get tricked here's the tricky puzzle and then also with my custom instructions it takes time discussing it and this time it gets it correct it takes the Cabbage across first so it took a lot of effort to get to a point where it could actually solve this because yeah when it's you know for things where it's been primed to answer a certain way again and again and again it's very hard for it to not do that okay now uh something else super helpful that you can use is what they call Advanced Data analysis in Advanced Data analysis you can ask it 