everything that you've passed into so far but then instead of adding in an assistant role response we have to provide a function role response and simply put in here the result we got back from the function and if we do that we now get the prose response 12 factorial is equal to 470 and a million 1 600. now functions like python you can still ask it about non-python things and it just ignores it if you don't need it right so you can have a whole bunch of functions available that you've built to do whatever you need for the stuff which um the language model isn't familiar with and it'll still solve whatever it can on its own and use your tools use your functions where possible okay so we have built our own code interpreter from scratch I think that's pretty amazing so that is um what you can do with or some of the stuff you can do with open AI um what about stuff that you can do on your own computer well to use a language model on your own computer you're going to need to use a GPU um so I guess the first thing to think about is like do you want this does it make sense to do stuff on your own computer what are the benefits um there are not any open source models that are as good yet as gpt4 and I would have to say also like actually open ai's pricing's really pretty good so it's it's not immediately obvious that you definitely want to kind of go in-house but there's lots of reasons you might want to and we'll look at some examples of them today one example you might want to go in-house is that you want to be able to ask questions about your proprietary documents or about information after September 2021 the the knowledge cut off or you might want to create your own model that's particularly good at solving the kinds of problems that you need to solve using fine tuning and these are all things that you absolutely can get better than GPT for performance at work or at home without too much without too much money or travel so these are the situations in which you might want to go down this path and so you don't necessarily have to buy a GPU on kaggle they will give you a notebook with two quite old gpus attached and very little Ram but it's something or you can use collab and on collab you can get much better gpus than kaggle has and more RAM particularly if you pay a monthly subscription fee um so those are some options for free or low cost you can also of course you know go to one of the many kind of GPU server providers and they change all the time is to kind of what's what's good or what's not run pod is one example and you can see you know if you want the biggest and best machine you're talking 34 an hour so it gets pretty expensive but you can certainly get things a lot cheaper 80 cents an hour um Lambda Labs is often pretty good um you know it's really hard at the moment to actually find um let's see pricing to actually find people that have them available so they've got lots listed here but they often have nine or very few available um there's also something pretty interesting called Fast AI which basically lets you use um other people's computers when they're not using them and as you can see you know they tend to be much cheaper than other folks and they they tend to have better availability as well but of course for sensitive stuff you don't want to be running it on some randos computer so anyway so there's a few options for renting stuff um you know I think it's if you can it's worth buying something and definitely the one to buy at the moment is the GTX 3090 used you can generally get them from eBay for like 700 bucks or so um a 40 90 isn't really better for language models even though it's a newer GPU the reason for that is that language models are all about memory speed how quickly can you get in and stuff in and out of memory rather than how fast is the processor and that hasn't really improved a whole lot so the two thousand bucks hmm the other thing as well as memory speed is memory size 24 gigs it doesn't quite cut it for a lot of things so you'd probably want to get two of these gpus so you're talking like fifteen hundred dollars or so um or you can get a 48 gig ram GPU it's called an a6000 but this is going to cost you more like five grand so again getting two of these is going to be a better deal and this is not going to be faster than these either um or funnily enough you could just get a Mac with a lot of ram particularly if you get an M2 Ultra Max have um particularly the M2 Ultra has pretty fast memory it's still going to be way slower than using an Nvidia card but it's going to be like you're going to be able to get you know like I think 192 gig or something um so it's not a terrible option particularly if you're not training models you're just wanting to use other existing trained models um so anyway most people who do this stuff seriously almost everybody has in video cards um so then what we're going to be using is a library called Transformers from hugging face and the reason for that is that basically people upload lots of pre-trained models or firetrained models up to the hugging face Hub and in fact there's even a leaderboard where you can see which are the best models now this is a really uh fraud area so at the moment this one is meant to be the best model it has the highest average score and maybe it is good I haven't actually 